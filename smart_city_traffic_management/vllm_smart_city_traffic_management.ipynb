{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "455b4e9a",
   "metadata": {},
   "source": [
    "### Install packages: Kafka, Postgres, NLP models, PyTorch, async tools, sentence embeddings, LLM access, vector search, progress bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c88662-663d-4741-92b9-6ea7b1aebea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip -q install kafka-python psycopg2-binary transformers torch asyncio aiokafka sentence-transformers litellm nest_asyncio pgvector tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2773d6",
   "metadata": {},
   "source": [
    "### This section imports the necessary Python modules and performs initial setup for our data science tasks. These include modules for interacting with Kafka, PostgreSQL (with vector support), generating text embeddings, performing sentiment analysis, and general asynchronous operations.\n",
    "\n",
    "**Purpose:** Prepare the environment for data ingestion, processing, and storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecf8763e-7e89-4641-9b40-099180d025be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "import logging\n",
    "import traceback  # Add this line\n",
    "\n",
    "from aiokafka import AIOKafkaProducer, AIOKafkaConsumer\n",
    "from kafka.errors import KafkaError\n",
    "import psycopg2\n",
    "from pgvector.psycopg2 import register_vector  # For registering vector type                 # Correct import for Vector class\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "import requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b1e580",
   "metadata": {},
   "source": [
    "### This section defines the configuration parameters for various components of the system, including Kafka, PostgreSQL, vllm and other settings relevant for data processing. We will use environment variables where applicable for sensitive information like database credentials and api keys\n",
    "\n",
    "**Purpose:**  Configure external services and application parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acff8148-8323-42c8-bf10-67f70cc88364",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Kafka configuration\n",
    "KAFKA_TOPIC = \"traffic_data\"\n",
    "KAFKA_BROKER = \"kafka-kafka-brokers.smart-city-traffic-management.svc.cluster.local:9092\"\n",
    "\n",
    "# PostgreSQL configuration (use environment variables or replace with your actual values)\n",
    "PG_HOST = os.getenv('PG_HOST', 'postgresql.pgvector.svc.cluster.local')\n",
    "PG_PORT = os.getenv('PG_PORT', '5432')\n",
    "PG_USER = os.getenv('PG_USER', 'vectordb')\n",
    "PG_PASSWORD = os.getenv('PG_PASSWORD', 'vectordb')\n",
    "PG_DATABASE = os.getenv('PG_DATABASE', 'traffic_data')\n",
    "\n",
    "# vllm endpoint configuration\n",
    "VLLM_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\"  # Replace with your model name\n",
    "\n",
    "# Set the vllm API key\n",
    "os.environ[\"VLLM_API_KEY\"] = \"your-vllm-api-key\"  # Update as needed\n",
    "\n",
    "# Set the vllm endpoint\n",
    "os.environ[\"VLLM_API_BASE\"] = \"http://vllm.vllm-gpu.svc.cluster.local:8000\"  # Adjust the endpoint as necessary\n",
    "VLLM_ENDPOINT = os.getenv('VLLM_API_BASE', 'http://vllm.vllm-gpu.svc.cluster.local:8000')\n",
    "\n",
    "# Number of iterations for the main loop\n",
    "ITERATIONS = 3  # Adjust as needed\n",
    "\n",
    "INTERVAL = 1  # Time interval between iterations in seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20285964",
   "metadata": {},
   "source": [
    "### Explanation of `generate_traffic_data`\n",
    "\n",
    " This function creates a dictionary representing a snapshot of traffic conditions:\n",
    "\n",
    " *   **Location:** Randomly selects a location type (Downtown, Suburbs, Airport, Train Station).\n",
    " *   **Road Transportation:**\n",
    "     *   `TrafficVolume:`  A random integer between 0 and 100.\n",
    "     *   `TrafficSpeed:`  A random float between based off a inverse relationship with `TrafficVolume`\n",
    "     *   `PublicTransitPassengers:`  Randomly generated with more passengers during \"Peak\" hours.\n",
    " *   **Time of Day:**  Randomly choose between \"Peak,\" \"Off-Peak,\" and \"Night.\"\n",
    " *   **Weather Conditions:**  Randomly selects a weather condition.\n",
    "\n",
    " The simulated data is returned as a Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a500b23d-fb82-4fa6-9df0-7128451c8093",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_traffic_data():\n",
    "    location_types = [\"Downtown\", \"Suburbs\", \"Airport\", \"Train Station\"]\n",
    "    time_of_day = random.choice([\"Peak\", \"Off-Peak\", \"Night\"])\n",
    "    weather_conditions = [\"Sunny\", \"Cloudy\", \"Raining\", \"Snowing\"]\n",
    "\n",
    "    traffic_volume = random.randint(0, 100)\n",
    "    traffic_speed = max(0, random.uniform(60 - (traffic_volume * 0.6), 60))\n",
    "    public_transit_passengers = (\n",
    "        random.randint(50, 100) if time_of_day == \"Peak\" else random.randint(0, 50)\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"Location\": random.choice(location_types),\n",
    "        \"Road Transportation\": {\n",
    "            \"TrafficVolume\": traffic_volume,\n",
    "            \"TrafficSpeed\": traffic_speed,\n",
    "            \"PublicTransitPassengers\": public_transit_passengers,\n",
    "        },\n",
    "        \"Time of Day\": time_of_day,\n",
    "        \"Weather Conditions\": random.choice(weather_conditions)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ce42d0",
   "metadata": {},
   "source": [
    "### The vllm_generate_insights function generates insights from traffic data using the vLLM model. It takes a list of traffic data summaries and a custom prompt, concatenates them, and sends the prompt to the vLLM model endpoint to generate insights. The generated insights are then logged and printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63651e45-a35a-4460-be03-03301f225d4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vllm_generate_insights(similar_data_texts, custom_prompt):\n",
    "    # Concatenate the texts\n",
    "    concatenated_text = '\\n'.join(similar_data_texts)\n",
    "    prompt = f\"Based on the following traffic data summaries:\\n{concatenated_text}\\n{custom_prompt}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            VLLM_ENDPOINT,  # Updated to use VLLM_ENDPOINT\n",
    "            json={\n",
    "                \"model\": VLLM_MODEL,  # Updated to use VLLM_MODEL\n",
    "                \"prompt\": prompt,\n",
    "                \"max_length\": 500,\n",
    "                \"num_return_sequences\": 1\n",
    "            }\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        generated_text = response.json().get('generated_text', '')  # Added default value in case key is absent\n",
    "        logger.info(\"VLLM Model Generated Insight:\")\n",
    "        logger.info(generated_text)\n",
    "        print(\"VLLM Model Generated Insight:\")\n",
    "        print(generated_text)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error querying vllm endpoint: {e}\")  # Updated log message\n",
    "        print(\"Error generating insight with VLLM model.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7fa839",
   "metadata": {},
   "source": [
    "### Kafka Producer\n",
    "\n",
    "#### This section contains the function responsible for asynchronously producing messages to a Kafka topic.\n",
    "\n",
    "**Environment:**  OpenShift AI (assumed)\n",
    "**Purpose:**  Send traffic data messages to a Kafka topic for stream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc1d6bf0-1a5f-447f-9bb5-b6ec789113e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def produce_to_kafka(producer, data):\n",
    "    try:\n",
    "        key = str(data[\"Time of Day\"]).encode()\n",
    "        await producer.send_and_wait(KAFKA_TOPIC, json.dumps(data).encode(), key=key)\n",
    "        logger.info(f\"Data sent to Kafka: {data}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error sending data to Kafka: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2be1f99",
   "metadata": {},
   "source": [
    "### Data Processing and Storage\n",
    "\n",
    "#### This section focuses on processing incoming traffic data, generating embeddings, and storing both the raw data and embeddings in a PostgreSQL database.\n",
    "\n",
    "  **Environment:** OpenShift AI (assumed)\n",
    "  **Purpose:**\n",
    "\n",
    "      *   Transform raw traffic data into a text format.\n",
    "      *   Generate text embeddings using a Sentence Transformer model.\n",
    "      *   Store the raw data and embeddings in a PostgreSQL database for analysis and similarity search.\n",
    "      \n",
    "  **Assumptions:**\n",
    "  \n",
    "      *   A PostgreSQL database is deployed and accessible within the OpenShift cluster, and the necessary table (`traffic_data`) has been created with columns for `data` (JSONB) and `embedding` (VECTOR).\n",
    "      *   An embedding model from the `sentence-transformers` library is initialized and available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20906bf4-ced9-4166-ba24-84e89c1c02d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def process_traffic_data(data, cur, conn, embedding_model):\n",
    "    text = (\n",
    "        f\"Location: {data['Location']}, \"\n",
    "        f\"Traffic volume: {data['Road Transportation']['TrafficVolume']}, \"\n",
    "        f\"Speed: {data['Road Transportation']['TrafficSpeed']:.2f}, \"\n",
    "        f\"Public transit passengers: {data['Road Transportation']['PublicTransitPassengers']}, \"\n",
    "        f\"Time: {data['Time of Day']}, \"\n",
    "        f\"Weather: {data['Weather Conditions']}\"\n",
    "    )\n",
    "\n",
    "    # Generate embedding\n",
    "    embedding = embedding_model.encode(text).tolist()\n",
    "\n",
    "    # Store data and embedding in PostgreSQL\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO traffic_data (data, embedding)\n",
    "        VALUES (%s, %s)\n",
    "        \"\"\",\n",
    "        (json.dumps(data), embedding)\n",
    "    )\n",
    "    conn.commit()\n",
    "\n",
    "    logger.info(f\"Processed and stored data: {data}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f37788",
   "metadata": {},
   "source": [
    "### Similarity Search in PostgreSQL\n",
    "\n",
    "This section defines a function to query the PostgreSQL database for similar traffic data based on vector similarity search using embeddings.\n",
    "\n",
    "**Environment:** OpenShift AI (assumed)\n",
    "**Purpose:** Retrieve the top *k* most similar traffic data entries from the database based on a given embedding vector. This leverages PostgreSQL's vector similarity search capabilities.\n",
    "\n",
    "**Assumptions:**\n",
    "  *   A PostgreSQL database is running and accessible, with a `traffic_data` table containing `data` (JSONB) and `embedding` (VECTOR) columns.\n",
    "  *   The `embedding` column is of a vector type compatible with PostgreSQL's `<#>` operator for cosine similarity calculations (e.g., `pgvector`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21c2d7c1-dbfa-460d-8681-325708b523f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_similar_traffic_data(conn, embedding, top_k=5):\n",
    "    cur = conn.cursor()\n",
    "    # Execute vector similarity search\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        SELECT data, embedding <#> %s::vector AS distance\n",
    "        FROM traffic_data\n",
    "        ORDER BY embedding <#> %s::vector ASC\n",
    "        LIMIT %s\n",
    "        \"\"\",\n",
    "        (embedding, embedding, top_k)\n",
    "    )\n",
    "    results = cur.fetchall()\n",
    "    cur.close()\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952cec25",
   "metadata": {},
   "source": [
    "###   This function takes a list of traffic data summaries and a custom prompt, concatenates them, and sends the prompt to the vLLM model to generate insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67101e9b-cb26-49f1-9e59-39b389b7f409",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from vllm import Completion\n",
    "\n",
    "def vllm_generate_insights(similar_data_texts, custom_prompt):\n",
    "    \"\"\"\n",
    "    Generate insights from traffic data using the vLLM model.\n",
    "\n",
    "    This function takes a list of traffic data summaries and a custom prompt,\n",
    "    concatenates them, and sends the prompt to the vLLM model to generate insights.\n",
    "\n",
    "    Args:\n",
    "        similar_data_texts (list of str): A list of text summaries related to traffic data.\n",
    "        custom_prompt (str): A custom prompt provided by the user to guide the generation.\n",
    "\n",
    "    Returns:\n",
    "        None: The generated insights are printed to the console.\n",
    "\n",
    "    Example:\n",
    "        similar_data = [\n",
    "            \"Traffic was heavier than usual during rush hour.\",\n",
    "            \"Multiple accidents were reported on the main highway.\"\n",
    "        ]\n",
    "        prompt = \"What are the potential impacts on traffic flow?\"\n",
    "        vllm_generate_insights(similar_data, prompt)\n",
    "    \"\"\"\n",
    "    # Concatenate the texts\n",
    "    concatenated_text = '\\n'.join(similar_data_texts)\n",
    "\n",
    "    try:\n",
    "        response = Completion.create(\n",
    "            model=VLLM_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"{custom_prompt}\\n{concatenated_text}\"}],\n",
    "            max_tokens=500,\n",
    "            n=1\n",
    "        )\n",
    "        generated_text = response['choices'][0]['message']['content']\n",
    "        print(\"vLLM Model Generated Insight:\")\n",
    "        print(generated_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating insight with vLLM model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d77d18",
   "metadata": {},
   "source": [
    "### Kafka Consumer and Data Processing Loop\n",
    "This section defines the asynchronous function that consumes messages from a Kafka topic, processes them, and stores the data and embeddings in PostgreSQL.\n",
    "\n",
    "**Purpose:** Continuously consume traffic data messages from Kafka, generate embeddings, perform similarity searches, get LLM insights, and store results. The function is designed to run asynchronously for efficient processing of streaming data.\n",
    "\n",
    "**Dependencies:** Requires the `aiokafka` library for Kafka interaction, `psycopg2` for PostgreSQL interaction, `sentence-transformers` for embedding generation, and an vllm LLM (or another model via `litellm`).\n",
    "\n",
    "**Assumptions:**\n",
    "  *   Kafka broker, PostgreSQL database, and vllm LLM are running and configured.\n",
    "  *   The `traffic_data` table exists in PostgreSQL.\n",
    "  *   `KAFKA_TOPIC`, `vllm_MODEL`, `EMBEDDING_MODEL_NAME`, and other relevant environment variables are set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36c5ddeb-f481-4ba3-98e3-b94d9acba942",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def consume_messages(consumer, cur, conn, embedding_model, num_messages):\n",
    "    count = 0\n",
    "    async for msg in consumer:\n",
    "        data = json.loads(msg.value.decode('utf-8'))\n",
    "        await process_traffic_data(data, cur, conn, embedding_model)\n",
    "        count += 1\n",
    "        if count >= num_messages:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35441ada",
   "metadata": {},
   "source": [
    "### Kafka Topic Creation\n",
    "\n",
    "This section defines a function to create a Kafka topic if it doesn't already exist. This is useful for setting up the necessary Kafka infrastructure for your data pipeline.\n",
    "\n",
    "**Purpose:**  Ensures that the required Kafka topic exists before starting the data streaming process. This simplifies deployment and avoids errors related to missing topics.\n",
    "**Dependencies:**  `kafka-python` library. Make sure it's installed (`pip install kafka-python`).\n",
    "\n",
    "**Assumptions:**\n",
    "  *   Kafka broker is running and accessible.\n",
    "  *   The `KAFKA_BROKER` environment variable is set with the correct broker address(es).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea94385d-8665-48e7-8eac-1e66854067f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "from kafka.errors import TopicAlreadyExistsError\n",
    "\n",
    "def create_kafka_topic(topic_name, num_partitions=1, replication_factor=1, bootstrap_servers=KAFKA_BROKER):\n",
    "    admin_client = KafkaAdminClient(bootstrap_servers=bootstrap_servers, client_id='test')\n",
    "    topic_list = [NewTopic(name=topic_name, num_partitions=num_partitions, replication_factor=replication_factor)]\n",
    "    try:\n",
    "        admin_client.create_topics(new_topics=topic_list, validate_only=False)\n",
    "        print(f\"Topic '{topic_name}' created successfully.\")\n",
    "    except TopicAlreadyExistsError:\n",
    "        print(f\"Topic '{topic_name}' already exists.\")\n",
    "    finally:\n",
    "        admin_client.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aba071",
   "metadata": {},
   "source": [
    "### Main Application Logic\n",
    "\n",
    "This section defines the main asynchronous function that orchestrates the entire data pipeline.\n",
    "\n",
    "\n",
    "**Purpose:** This function sets up the necessary connections (PostgreSQL, Kafka), initializes the embedding model, creates the Kafka topic, produces sample traffic data, and consumes/processes it. It serves as the entry point for the complete data pipeline.\n",
    "\n",
    "**Dependencies:**\n",
    "*   `aiokafka`: For asynchronous Kafka interaction.\n",
    "*   `psycopg2`:  For PostgreSQL database interaction.\n",
    "*   `sentence-transformers`: For embedding generation.\n",
    "*   Other functions defined earlier:  `create_kafka_topic`, `produce_to_kafka`, `consume_messages`, `generate_traffic_data`, `process_traffic_data`, `query_similar_traffic_data`, `vllm_generate_insights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5eef45-674b-4e1d-9c79-3ec5b7cc3e02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def main():\n",
    "    # Connect to PostgreSQL\n",
    "    conn = psycopg2.connect(\n",
    "        host=PG_HOST,\n",
    "        port=PG_PORT,\n",
    "        user=PG_USER,\n",
    "        password=PG_PASSWORD,\n",
    "        database=PG_DATABASE\n",
    "    )\n",
    "    register_vector(conn)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Create the pgvector extension\n",
    "    cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n",
    "    conn.commit()\n",
    "\n",
    "    # Create table for traffic data if it doesn't exist\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS traffic_data (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            data JSONB,\n",
    "            embedding vector(384)\n",
    "        )\n",
    "        \"\"\"\n",
    "    )\n",
    "    conn.commit()\n",
    "\n",
    "    # Initialize embedding model\n",
    "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # Dimension: 384\n",
    "\n",
    "    # Create Kafka topic if it doesn't exist\n",
    "    create_kafka_topic(KAFKA_TOPIC)\n",
    "\n",
    "    # Create Kafka producer\n",
    "    producer = AIOKafkaProducer(bootstrap_servers=[KAFKA_BROKER])\n",
    "\n",
    "    await producer.start()\n",
    "\n",
    "    try:\n",
    "        for _ in range(ITERATIONS):\n",
    "            # Generate and produce traffic data\n",
    "            traffic_data = generate_traffic_data()\n",
    "            await produce_to_kafka(producer, traffic_data)\n",
    "            await asyncio.sleep(INTERVAL)\n",
    "\n",
    "        # Consume and process traffic data\n",
    "        consumer = AIOKafkaConsumer(\n",
    "            KAFKA_TOPIC,\n",
    "            bootstrap_servers=[KAFKA_BROKER],\n",
    "            auto_offset_reset='earliest',\n",
    "            enable_auto_commit=True,\n",
    "            group_id='traffic-consumer-group'\n",
    "        )\n",
    "        await consumer.start()\n",
    "        try:\n",
    "            # Adjust the number of messages to consume\n",
    "            await consume_messages(consumer, cur, conn, embedding_model, ITERATIONS)\n",
    "        finally:\n",
    "            await consumer.stop()\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred in main: {e}\")\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        await producer.stop()\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        logger.info(\"Main function execution completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af75a40",
   "metadata": {},
   "source": [
    "### Run the main function to start the data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e3e93b-7cb5-4a7a-b288-82646a7cb24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the main function\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c8c148",
   "metadata": {},
   "source": [
    "### Standalone Traffic Data Query and Analysis\n",
    "\n",
    "This section provides a standalone script to query the PostgreSQL database for similar traffic data based on a sample text input and then generate insights using the vllm LLM.\n",
    "\n",
    "\n",
    "**Purpose:**  This script allows users to manually input a description of traffic conditions, find similar historical data in the database, and get insights from an LLM (vllm via `litellm`). It's useful for ad-hoc analysis and testing the similarity search and insight generation components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e19b5b2-0a75-4ed9-b36f-791b8dcec14c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time  # For simulating processing time\n",
    "\n",
    "# After data processing, perform the vllm model query\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    host=PG_HOST,\n",
    "    port=PG_PORT,\n",
    "    user=PG_USER,\n",
    "    password=PG_PASSWORD,\n",
    "    database=PG_DATABASE\n",
    ")\n",
    "\n",
    "# Generate an embedding for a sample query\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Sample text describing traffic conditions\n",
    "# City managers can modify this text to test different traffic scenarios, such as:\n",
    "# - \"Heavy congestion on Main Street during morning rush hour due to construction.\"\n",
    "# - \"Increased pedestrian activity near Central Park on weekends.\"\n",
    "# - \"Accident on Highway 50 causing significant delays in both directions.\"\n",
    "# - \"Road closures downtown for the annual marathon event.\"\n",
    "# - \"Snowstorm expected to impact traffic flow in the northern districts.\"\n",
    "#sample_text = \"High traffic volume in downtown during peak hours with raining conditions.\"\n",
    "sample_text = \"Accident on Highway 50 causing significant delays in both directions.\"\n",
    "custom_prompt=\"\"\n",
    "# Display progress bar for embedding generation\n",
    "with tqdm(total=1, desc=\"Generating embedding\") as pbar:\n",
    "    sample_embedding = embedding_model.encode(sample_text).tolist()\n",
    "    pbar.update(1)\n",
    "\n",
    "# Query similar traffic data\n",
    "similar_data = query_similar_traffic_data(conn, sample_embedding, top_k=5)\n",
    "\n",
    "# Extract the data texts\n",
    "similar_data_texts = []\n",
    "for row in similar_data:\n",
    "    data = row[0]\n",
    "    data_text = (\n",
    "        f\"Location: {data['Location']}, \"\n",
    "        f\"Traffic volume: {data['Road Transportation']['TrafficVolume']}, \"\n",
    "        f\"Speed: {data['Road Transportation']['TrafficSpeed']:.2f}, \"\n",
    "        f\"Public transit passengers: {data['Road Transportation']['PublicTransitPassengers']}, \"\n",
    "        f\"Time: {data['Time of Day']}, Weather: {data['Weather Conditions']}\"\n",
    "    )\n",
    "    similar_data_texts.append(data_text)\n",
    "\n",
    "# Use vllm model to generate insights\n",
    "# Display progress bar while waiting for response\n",
    "with tqdm(total=1, desc=\"Generating insights with vllm\") as pbar:\n",
    "    vllm_generate_insights(similar_data_texts,custom_prompt)\n",
    "    pbar.update(1)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb503c21-81c6-43b3-ad15-b708f2fef580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c10aa0-8ede-4358-847e-a65849b47ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
